{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append('./source')\n",
    "import mgc_adversary\n",
    "import mgc_classifier\n",
    "import dataset_loader as datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'/home/azureuser/MGC_classifier'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AZURE\n",
    "To get the dataset I need to point to the datastore object somewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "mgc_adversary = reload(mgc_adversary)\n",
    "mgc_classifier = reload(mgc_classifier)\n",
    "datasets = reload(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "class train_and_validate_adversarial:\n",
    "    \"\"\"\n",
    "    class that implements training the network and\n",
    "    outputting validation metrics\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, model, datapath, criterion, optimizer, lrs, batch_size, minibatch_size,\n",
    "        num_workers, scheduler,\n",
    "    ):\n",
    "\n",
    "        # choose gpu or cpu\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(\"USING: \", self.device)\n",
    "        # initialize the model\n",
    "        self.model = model(\n",
    "            out_channels=1, \n",
    "            num_conv_layers=5, \n",
    "            n_classes=40, \n",
    "            img_size=32,\n",
    "            num_dense_layers = 4, \n",
    "            kernel = 3\n",
    "        ).to(self.device)\n",
    "        self.datapath = datapath\n",
    "        self.optimizer = optimizer(self.model.parameters(), lr=lrs[1])\n",
    "        self.criterion = criterion.to(self.device)\n",
    "        self.scheduler = scheduler(\n",
    "            self.optimizer, \n",
    "            base_lr=lrs[0], \n",
    "            max_lr=lrs[2],\n",
    "            cycle_momentum=False\n",
    "        )\n",
    "        \n",
    "        print(self.model)\n",
    "\n",
    "        # hyperparameters\n",
    "        self.batch_size = batch_size\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        # initialize the logging variables\n",
    "        self.training_loss = []\n",
    "        self.validation_loss = []\n",
    "        self.validation_accuracy = []\n",
    "        self.class_accuracy = np.zeros(40)\n",
    "\n",
    "        # training and validation dataloaders\n",
    "        self.train_ratio = 0.99\n",
    "        self.train_dataloader = None\n",
    "        self.validation_dataloader = None\n",
    "        \n",
    "        # get the full dataset in the folder\n",
    "        folder_dataset = datasets.PklDataset(self.datapath)\n",
    "        print(len(folder_dataset))\n",
    "\n",
    "        # split data into training and test\n",
    "\n",
    "        train_data, validation_data = torch.utils.data.random_split(\n",
    "            dataset=folder_dataset,\n",
    "            lengths=[\n",
    "                int(len(folder_dataset) * self.train_ratio),\n",
    "                len(folder_dataset) - int(len(folder_dataset) * self.train_ratio),\n",
    "            ],\n",
    "        )\n",
    "        print(len(train_data))\n",
    "\n",
    "        # get the DataLoaders\n",
    "        self.train_dataloader = DataLoader(dataset = train_data,\n",
    "            batch_size = self.batch_size,\n",
    "            shuffle = True, \n",
    "            collate_fn = datasets.PklDataset.collate_fn,\n",
    "        )\n",
    "        self.validation_dataloader = DataLoader(dataset = validation_data,\n",
    "            batch_size = self.batch_size,\n",
    "            shuffle = True, \n",
    "            collate_fn = datasets.PklDataset.collate_fn,\n",
    "        )\n",
    "\n",
    "    def train(self, epochs=3):\n",
    "        \"\"\"\n",
    "        train the network\n",
    "        \"\"\"\n",
    "        for epoch in range(10):\n",
    "            # initialize the training loss averaging list\n",
    "            temp_loss = []\n",
    "            # iterate through the samples\n",
    "            ii = 0\n",
    "            for i, batch_sample in enumerate(self.train_dataloader):\n",
    "                minibatch = datasets.MinibatchDataset(\n",
    "                    data = batch_sample,\n",
    "                )\n",
    "                minibatch_dataloader = DataLoader(\n",
    "                    dataset = minibatch,\n",
    "                    batch_size = self.minibatch_size,\n",
    "                    shuffle = True,\n",
    "                    pin_memory=True,\n",
    "                )\n",
    "                self.optimizer.zero_grad()\n",
    "                for inputs, targets in minibatch_dataloader:\n",
    "\n",
    "                    # send minibatch to gpu or cpu\n",
    "                    inputs = inputs.to(self.device)\n",
    "                    targets = targets.to(self.device)\n",
    "\n",
    "                    # forward pass\n",
    "                    predictions = self.model(inputs)\n",
    "                    \n",
    "                    # loss \n",
    "                    loss = self.criterion(predictions, targets)\n",
    "                    \n",
    "\n",
    "                    # backward step\n",
    "                    loss.backward()\n",
    "                    \n",
    "                    self.optimizer.step()\n",
    "                    self.scheduler.step()\n",
    "\n",
    "                    self.training_loss.append(loss.item())\n",
    "                    \n",
    "                    print(f'\\r{i} {ii}', end = '')\n",
    "                    ii+=1\n",
    "                    \n",
    "                        \n",
    "                if i%25 == 0:\n",
    "                    self.validate()\n",
    "                    try:\n",
    "                        plot_metrics(self)\n",
    "                    except KeyboardInterrupt:\n",
    "                        break\n",
    "                    \n",
    "#                 if len(self.validation_loss)>10:\n",
    "#                     if self.validation_loss[-1]<self.validation_loss[-2]:\n",
    "#                         torch.save(\n",
    "#                             self.model.state_dict(), \n",
    "#                             Path(\"./drive/MyDrive/models/StaticMgcClassifier.pkl\")\n",
    "#                         )\n",
    "  \n",
    "    def validate(self):\n",
    "        \"\"\"\n",
    "        validate the network predictions\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # initalize list to average the accuracy\n",
    "            # and loss over the validation dataset\n",
    "            loss_temp = []\n",
    "            accuracy_temp = []\n",
    "            prediction_class_sum = np.zeros(40)\n",
    "            class_accuracy_temp = torch.zeros([7,self.minibatch_size, self.model.n_classes])\n",
    "            for i, batch_sample in enumerate(self.validation_dataloader):\n",
    "                minibatch = datasets.MinibatchDataset(\n",
    "                    data = batch_sample,\n",
    "                )\n",
    "                minibatch_dataloader = DataLoader(\n",
    "                    dataset = minibatch,\n",
    "                    batch_size = self.minibatch_size,\n",
    "                    shuffle = True,\n",
    "                    pin_memory=True,\n",
    "                )\n",
    "                for inputs, targets in minibatch_dataloader:\n",
    "\n",
    "                    # send minibatch to gpu or cpu\n",
    "                    inputs = inputs.to(self.device)\n",
    "\n",
    "                    # run prediction on a subset of the data\n",
    "                    predictions = self.model(inputs)\n",
    "                    predictions = predictions.cpu()\n",
    "\n",
    "                    # get validation loss\n",
    "                    loss_temp.append(self.criterion(predictions, targets).numpy().item())\n",
    "\n",
    "                    # get validation accuracy\n",
    "                    for j in range(predictions.shape[0]):\n",
    "                        tgts = targets[j, :]\n",
    "                        preds = predictions[j, :]\n",
    "                        k = int(torch.sum(tgts))\n",
    "                        prediction_topk = torch.topk(preds, k=k).indices\n",
    "                        prediction_classes = torch.zeros_like(preds)\n",
    "                        prediction_classes[prediction_topk] = 1.0\n",
    "                        number_correct_classes = torch.sum(torch.logical_and(tgts, prediction_classes))\n",
    "                        accuracy_temp.append((number_correct_classes/k).numpy().item())\n",
    "                        \n",
    "                        tgts=tgts.int()\n",
    "                        prediction_classes = prediction_classes.int()\n",
    "                        # for stacked bar plot\n",
    "                        # true and true\n",
    "                        class_accuracy_temp[0, :]+=torch.logical_and(tgts, prediction_classes).float().numpy()\n",
    "                        # false and false\n",
    "                        class_accuracy_temp[3, :]+=torch.logical_and(~tgts, ~prediction_classes).float().numpy()\n",
    "                        # predict true when false\n",
    "                        class_accuracy_temp[2, :]+=torch.logical_and(~tgts, prediction_classes).float().numpy()\n",
    "                        # predict false when true\n",
    "                        class_accuracy_temp[1, :]+=torch.logical_and(tgts, ~prediction_classes).float().numpy()\n",
    "                        # totals Trues\n",
    "                        class_accuracy_temp[4, :]+=tgts.float().numpy()\n",
    "                        # totals False\n",
    "                        class_accuracy_temp[5, :]+=tgts.float().numpy()\n",
    "                        # totals\n",
    "                        class_accuracy_temp[6, :]+=np.ones_like(tgts)\n",
    "                        \n",
    "            class_accuracy_temp=np.sum(np.array(class_accuracy_temp), axis=1)\n",
    "            self.class_accuracy = class_accuracy_temp\n",
    "            self.validation_accuracy.append(np.mean(accuracy_temp))\n",
    "            self.validation_loss.append(np.mean(loss_temp))\n",
    "\n",
    "def plot_metrics(training_object):\n",
    "    moving_average_over = 200\n",
    "    moving_window = -(100)\n",
    "    if len(training_object.training_loss)<moving_window:\n",
    "        moving_window=0\n",
    "    clear_output(wait=True)\n",
    "    # initialize figure for plotting\n",
    "    fig, axs = plt.subplots(nrows=3, ncols=1)\n",
    "    fig.set_size_inches(8.5, 10.5)\n",
    "    axs[0].plot(moving_average(training_object.training_loss, 200))\n",
    "    axs[0].legend(['train loss'])\n",
    "    axs[1].plot(training_object.validation_loss[moving_window:], 'orange')\n",
    "    axs[1].legend(['val_loss'], loc = 'upper left')\n",
    "    ax1t = axs[1].twinx()\n",
    "    ax1t.plot(training_object.validation_accuracy[moving_window:], 'g')\n",
    "    ax1t.legend(['val recall'], loc= 'lower left')\n",
    "    # barplot\n",
    "    for i in [0,1,2,3]:\n",
    "        if i==0:\n",
    "            bottom=np.zeros_like(training_object.class_accuracy[5,:])\n",
    "        else:\n",
    "            bottom+=training_object.class_accuracy[i-1,:]\n",
    "        colors = ['g','yellow','darkorange', 'c']\n",
    "        labels = ['TruPos', 'FalseNeg', 'FalsePos', 'TruNeg']\n",
    "        axs[2].bar(\n",
    "            x = datasets.PklDataset(0).categories,\n",
    "            height = training_object.class_accuracy[i,:],\n",
    "            bottom=bottom,\n",
    "            color = colors[i],\n",
    "            label=labels[i]\n",
    "        )\n",
    "    axs[2].legend()\n",
    "    axs[2].tick_params(axis='x', labelrotation=90)  \n",
    "    axs[2].set_yscale('log')\n",
    "    plt.pause(0.0001)\n",
    "            \n",
    "def moving_average(x, w):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do - compare MultiLabelSoftMarginLoss() and BCELoss accuracy performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "mgc_adversary = reload(mgc_adversary)\n",
    "mgc_classifier = reload(mgc_classifier)\n",
    "datasets = reload(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING:  cuda\n",
      "MgcNet(\n",
      "  (conv_input): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv0): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (maxpool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (conv1): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (maxpool2): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (maxpool3): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "  (linear0): Linear(in_features=1024, out_features=40, bias=True)\n",
      "  (linear1): Linear(in_features=40, out_features=40, bias=True)\n",
      "  (linear2): Linear(in_features=40, out_features=40, bias=True)\n",
      "  (linear3): Linear(in_features=40, out_features=40, bias=True)\n",
      "  (batch_normalization): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU()\n",
      "  (softmax): Softmax(dim=1)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e2fe67a5183d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mroot_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m train_val_object = train_and_validate_adversarial(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmgc_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMgcNet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdatapath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-9b9087407ab7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, datapath, criterion, optimizer, lrs, batch_size, minibatch_size, num_workers, scheduler)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# get the DataLoaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         self.train_dataloader = DataLoader(dataset = train_data,\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers)\u001b[0m\n\u001b[1;32m    264\u001b[0m                     \u001b[0;31m# Cannot statically verify that dataset is Sized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                     \u001b[0;31m# Somewhat related: see NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequentialSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torch/utils/data/sampler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_samples\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             raise ValueError(\"num_samples should be a positive integer \"\n\u001b[0m\u001b[1;32m    104\u001b[0m                              \"value, but got num_samples={}\".format(self.num_samples))\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "test_model = mgc_classifier.MgcNet\n",
    "\n",
    "root_path = Path(\"./data\")\n",
    "train_val_object = train_and_validate_adversarial(\n",
    "    model=mgc_classifier.MgcNet,\n",
    "    datapath=root_path,\n",
    "    criterion=torch.nn.MultiLabelSoftMarginLoss(),\n",
    "    optimizer=torch.optim.Adam,\n",
    "    scheduler=torch.optim.lr_scheduler.CyclicLR,\n",
    "    lrs=[1e-8, 1e-5, 1e-5],\n",
    "    batch_size=15,  # just how much data can be loaded into memory at one time\n",
    "    minibatch_size=5,  # what actually controls the batching size for training\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "train_val_object.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}